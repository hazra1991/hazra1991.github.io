---
layout: post  
title: "OpenTelemetry (OTel): Unified Observability, Simplified (Part 1)"
date: 2025-08-17
categories: ['tutorial','OpenTelemetry (OTel)']
tags: ['OTel','telemetry',tutorial]
slug: openTelemetry-unified-observability-part1
image:
  path: assets/images/covers/openTelemetry_part1.png
  alt: OpenTelemetry (OTel)
---

<details>
  <summary><strong>Table of Contents</strong></summary>

  <div markdown="1">

  - [Telemetry?](#telemetry)
  - [OpenTelemetry (OTel)?](#opentelemetry-otel)
  - [Why Do We Need OpenTelemetry (OTel)?](#why-do-we-need-opentelemetry-otel)
  - [How OTel Solves It?](#how-otel-solves-it)
  - [What OpenTelemetry is (Nutshell)](#what-opentelemetry-is-nutshell)
  - [OTel's Three Pillars: traces, metrics, logs (With Schemas)](#otels-three-pillars-traces-metrics-logs-with-schemas)
    - [Traces üïµÔ∏è ‚Äî What happened, in what order](#traces-anchor)
      - [schema](#trace-schema)
    - [Metrics üìà ‚Äî How much, how often, how long](#metrics-anchor)
      - [schema](#metric-schema)
    - [Logs üìú ‚Äî What was said](#logs-anchor)
      - [schema](#log-schema)
  - [Conclusion](#conclusion)
  - [For further understanfding of click here ]
  </div>
</details>

## Telemetry?

Telemetry is the collection, transmission, and measurement of data from remote systems or devices ‚Äî so you can observe and analyze how they‚Äôre performing without being physically present.

In software and IT, telemetry refers to the data collected from applications, services, and infrastructure to understand their behavior, health, and performance.

## OpenTelemetry (OTel)?

OpenTelemetry (OTel) is an open-source observability a **Standard** + **protocol** + **implementation (SDKs)** incubated by the Cloud Native Computing Foundation (CNCF), designed to provide a unified approach for generating, collecting, managing, and exporting telemetry data from software applications.
>Think of it like HTTP: HTTP has a standard (RFCs), and any server/client that speaks HTTP can talk to each other. You don‚Äôt have to re-implement HTTP yourself ‚Äî you use a library like `curl`, `requests`, etc., or language-specific SDKs in OTel that help take away the hassle of implementing those standards manually.
{: .prompt-tip}

> Telemetry data in OpenTelemetry includes `traces`, `metrics`, and `logs` ‚Äî `the three fundamental pillars of observability`.
{: .prompt-tip}

## Why Do We Need OpenTelemetry (OTel)?  

In modern systems, we want to understand what‚Äôs happening inside our applications ‚Äî how they behave, how they perform, where issues occur, and what users are experiencing. This understanding comes from telemetry data ‚Äî `traces`, `metrics`, and `logs` ‚Äî generated by the application itself **(By developer or may be using custome logging if we decide to log a specific event or something**).

Let‚Äôs break this down to understand the problem and how OTel solves it.

>**For simplification:**
>- **Telemetry Producers**: Apps and services that generate telemetry data (`logs`, `traces`, `metrics`).
>- **Telemetry Backends**: Tools that ingest telemetry data to provide value ‚Äî e.g., visualize metrics (**Grafana**), alert on anomalies (**Prometheus**), trace requests (**Jaeger**), or monitor infrastructure (**Datadog**).

### The Nature of Telemetry Data (Before OpenTelemetry)
Before diving deeper, it's important to understand that telemetry data isn‚Äôt inherently structured or tied to any specific format. It‚Äôs simply a concept ‚Äî data that helps us understand what‚Äôs happening inside our applications.

This data can be as simple as logging a message like:

> "App was called" ‚Äî every time the application is triggered,

Or as complex as recording a full, structured trace of the entire request lifecycle:

> App was called ‚Üí hit the database ‚Üí created a user ‚Üí failed at a validation step ‚Üí took 120ms total latency,  
> along with timestamps, status codes, error messages, and much more.

>**The point is:**  
>There‚Äôs no enforced or universal way to generate or structure telemetry data.  
We, as developers or a vendor, can log, trace, and structure and format telemetry data however they choose ‚Äî which leads to inconsistencies between different applications and systems.
{: .prompt-info}

To make things more difficult, if you're using a vendor-specific `backend` (like Datadog, New Relic, etc.), you often have to follow their telemetry structure ‚Äî even if they provide SDKs. This creates tight coupling and a bottleneck:which is often refer to as **vendor lock**

### Now imagine the problem.

Each application or service produces different telemetry data, or each vendor demands telemetry data in a different structure, using its own JSON format, naming conventions, and data flow styles. What a mess.

For example:

App A logs traces like:

```json
{ "event": "db_query", "time": 123, "user": "42" }
```
App B logs:

```json
{ "span": "query", "timestamp": 123, "uid": "42" }
```

Or, one vendor might need log like this:

```json
{ "event": "app_called", "timestamp": "2025-08-18T10:00:00Z" }
```
While another might need log:

```json
{ "action": "user_created", "latency": 250, "db": "hit", "status": "success" }
```

üò¨ If you send these app data to a dashboard (Datadog, Jaeger, New Relic, etc.), the backend can‚Äôt interpret both formats the same way. You‚Äôd need custom parsers for each format ‚Äî **a huge mess**. üòµ‚Äçüí´  
ü§Ø Or, if you comply with vendor-specific formats, you have to learn and maintain them all ‚Äî **a huge mess**. üòµ‚Äçüí´


### How OTel Solves it?

OpenTelemetry defines a standard schema + APIs (to produce those schemas without needing to understand them) for `traces`, `metrics`, and `logs`.

- Every language (Python, Go, Java, etc.) implements the same OTel spec(usually using the OTel SDKs).
- Your app creates telemetry using OTel APIs (SDKs).
- That telemetry has a well-defined structure.
- `Exporters` serialize it (often JSON + Protobuf over gRPC/HTTP) ‚Üí backend systems can read it (e.g., **Prometheus**, **Jaeger**, **Grafana**, **Dynatrace**, or any custom dashboard like **Langfuse**).
- Developers no longer have to worry about creating or maintaining custom telemetry structures.

>üëâ **Result:** One universal format ‚Äî every dashboard understands it. Every application sends it
{: .prompt-tip}

---

## What OpenTelemetry is (Nutshell)

You can think of OpenTelemetry (OTel) as three things combined:

* <details>
  <summary><strong>A Standard (Schema + Semantic Conventions) [click me]</strong></summary>

  <div markdown="1">

  Defines what telemetry looks like.(`trace`,`metrics`,`logs`)

  - Example: a trace must have `traceId`, `spanId`, `parentSpanId`, `name`, `timestamps`, `attributes`, etc.
  - The same applies for metrics and logs.
  - The keys are fixed by spec (you can‚Äôt rename `traceId` ‚Üí `tracingit`).
  - But you **can** add extra key-value pairs in `attributes`.

  </div> 
  </details>

<span id="as-a-protocol"></span>
* <details>
  <summary><strong>A Protocol (OTLP: OpenTelemetry Protocol) [click me]</strong></summary>

  <div markdown="1">

  Defines how telemetry is transmitted.

  - Serialization = **Protobuf** (binary, efficient) or **JSON** (human-readable, but less common in production).
  - Ensures exporters, collectors, and backends all speak the same wire format.
  
  Canonical schema:
  - OpenTelemetry defines its schema in Protobuf **`(trace.proto, metrics.proto, logs.proto)`**.
  - That‚Äôs the wire format used by default.

  JSON option:
  - The **OTLP protocol** also supports **JSON serialization** as an alternative.
  - Primarily used for **debugging** or **simple HTTP-based backends**.
  - ‚úÖ Yes, you can send the defined schema in JSON via HTTP (`Content-Type: application/json`), and compliant OTel backends should accept it.

  So ‚Äî OTel is not only Protobuf. JSON is official, but Protobuf is preferred for performance.
  [OpenTelemetry specification](https://github.com/open-telemetry/opentelemetry-proto/tree/main/opentelemetry/proto)

  </div> 
  </details>


* <details>
  <summary><strong>APIs + SDKs (Implementations) [click me]</strong></summary>

  <div markdown="1">

  Let you instrument your code without worrying about schema details.

  **Example in Python:**

  ```python
  with tracer.start_as_current_span("http_request") as span:
      span.set_attribute("http.method", "GET")
  ```

  </div> 
  </details>

---

{% include embed/youtube.html id='iEEIabOha8U' %}

## OTel's Three Pillars: `traces`, `metr ics`, `logs` (With Schemas) {#otels-three-pillars-traces-metrics-logs-with-schemas}

OpenTelemetry standardizes three core types of telemetry signals, often referred to as the "three pillars of observability" or ‚Äútelemetry triad.‚Äù:
**OTel defines one canonical schema for each signal type defined below:**
- `Traces` ‚Äî *What happened, in what order*
- `Metrics` ‚Äî *How much, how often, how long*
- `Logs` ‚Äî *What was said*

>- These schemas are defined in **OTLP Protobuf files -> `.proto`** within the [OTel spec](https://github.com/open-telemetry/opentelemetry-proto/tree/main/opentelemetry/proto) , supporting both `protobuf` and `Json` formats.All compliant **SDK** and All compliant **backend** **must** use the same schema.
>- That‚Äôs why you can send telemetry from your **Python app** into tools like **Jaeger**,**Langfuse**, **Honeycomb**, and others ‚Äî and they all understand it out of the box.
{: .prompt-info}

> ‚ö†Ô∏è **Note:**  
> The schemas shown in the subsequent sections are **high-level representations**  
> and come **directly from the Protobuf definitions** ‚Äî just in **human-readable JSON**,not binary. Their structure strictly follows the official OTLP spec.
{: .prompt-warning }


### Traces üïµÔ∏è ‚Äî *What happened, in what order* {#traces-anchor}

1. `traces`
A **trace** is the complete story of a single workflow ‚Äî starting from a **root span**, followed by a series of **nested spans** (representing meaningful events or logged actions), and ending when the workflow completes.
- A full story of a workflow (e.g., one user request).  
- A trace tells the entire journey of that request across various services, functions, or components.It helps answer:üëâ What happened? Where did it go? What took the most time? Where did it fail?
- Composed of **spans**, which represent individual units of work or step.
*Example analogy:* A **movie**.

2. `Span`  
  A step inside the workflow.  
  - Each span has a start + end, a name, duration, and metadata.  
  - Each Span can be nested
  - **Why:** Breaks down the workflow into understandable chunks.  
  üëâ *Example analogy:* A **scene** in the movie (may followed by some special effects).

>**Note** here during nesting the trace ID or the contex is automatically passed to the child span if we are using OTel SDKs or else if not using SDK (which is very bad) we have to pass the Trace ID (the context) . context propagation is auto done with the SDKs
{: .prompt-tip}

**Example: Tracing a User Signup**

Here‚Äôs how tracing would look for a user registration workflow:
**Trace**: User Signup Request

```markdown
Trace: User Signup Request
‚îî‚îÄ‚îÄ Span : HTTP POST /signup (root span)
    ‚îú‚îÄ‚îÄ Span : Validate input data
    ‚îú‚îÄ‚îÄ Span : Write user to database
    ‚îÇ   ‚îú‚îÄ‚îÄ Span: Connect to DB
    ‚îÇ   ‚îú‚îÄ‚îÄ Span: Execute INSERT query
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Event: Query success
    ‚îÇ   ‚îî‚îÄ‚îÄ Span: Close DB connection
    ‚îî‚îÄ‚îÄ Span : Send welcome email
        ‚îú‚îÄ‚îÄ Span: Generate email content
        ‚îú‚îÄ‚îÄ Span: Call email service API
        ‚îÇ   ‚îî‚îÄ‚îÄ Event: 202 Accepted (email queued)
        ‚îî‚îÄ‚îÄ Span: Log delivery status
# movie analogy 
Trace (full movie)  
 ‚îú‚îÄ‚îÄ Span (scene 1)  
 ‚îú‚îÄ‚îÄ Span (scene 2, special effect)  
 ‚îÇ     ‚îî‚îÄ‚îÄ Span (some event to capture)  
 ‚îî‚îÄ‚îÄ Span (scene 3)
```

<span id="trace-schema"></span>
**Trace Schema `trace.proto`**
  
<details>
  <summary><strong>The canonical structure is defined in file trace.proto. click to view it At a high level:</strong></summary>

  <div markdown="1">

  ```markdown
  ResourceSpans
  ‚îî‚îÄ‚îÄ resource (describes service: attributes like service.name, sdk.version)
  ‚îî‚îÄ‚îÄ scope_spans (groups by instrumentation library)
        ‚îî‚îÄ‚îÄ spans[] (the actual spans)
              ‚îú‚îÄ‚îÄ trace_id
              ‚îú‚îÄ‚îÄ span_id
              ‚îú‚îÄ‚îÄ parent_span_id
              ‚îú‚îÄ‚îÄ name
              ‚îú‚îÄ‚îÄ kind (INTERNAL, SERVER, CLIENT, PRODUCER, CONSUMER)
              ‚îú‚îÄ‚îÄ start_time_unix_nano
              ‚îú‚îÄ‚îÄ end_time_unix_nano
              ‚îú‚îÄ‚îÄ attributes[] (key/value custom metadata)
              ‚îú‚îÄ‚îÄ events[] (timestamped annotations)
              ‚îú‚îÄ‚îÄ links[] (span links across traces)
              ‚îú‚îÄ‚îÄ status (OK, ERROR, UNSET)
  ```
  {: file=trace.proto}

  </div> 
</details>

<details>
  <summary><strong>Here‚Äôs how the above look when serialized as JSON.[click to view]</strong></summary>

  <div markdown="1">

  ```json
  {
    "resourceSpans": [
      {
        "resource": {
          "attributes": [
            { "key": "service.name", "value": { "stringValue": "checkout-service" } }
          ]
        },
        "scopeSpans": [
          {
            "scope": { "name": "otel.instrumentation.http" },
            "spans": [
              {
                "traceId": "4fd0c9e6e9c6bfb1a5f7d29d6c72b78f",
                "spanId": "6f9c2d7f8c4d1f32",
                "parentSpanId": "0000000000000000",
                "name": "HTTP GET /api/users/42",
                "kind": "SPAN_KIND_SERVER",
                "startTimeUnixNano": "1692288000000000000",
                "endTimeUnixNano": "1692288001000000000",
                "attributes": [
                  { "key": "http.method", "value": { "stringValue": "GET" } },
                  { "key": "http.url", "value": { "stringValue": "/api/users/42" } },
                  { "key": "net.peer.ip", "value": { "stringValue": "192.168.1.10" } }
                ],
                "status": { "code": "STATUS_CODE_OK" }
              }
            ]
          }
        ]
      }
    ]
  }
  ```

  </div> 
</details>

>üëâ This is the one and only trace schema.
>All SDKs produce spans in this shape, and all backends consume them.

### Metrics üìà ‚Äî *How much, how often, how long* {#metrics-anchor}
Metrics are **aggregated, numerical data points** that represent performance, resource usage, or behavior over time.  
They are lightweight, ideal for **monitoring and alerting**, and provide a **high-level view of system health**.
Common Use Cases:
- How many HTTP requests per second is the app handling?
- What‚Äôs the 95th percentile response time?
- Is memory or CPU usage spiking?
- Examples: **latency histograms**, **request counts**, **CPU usage**, etc.


<span id="metric-schema"></span>
**Metric Schema `metrics.proto.`**
  
<details>
  <summary><strong>The canonical structure is Defined in metrics.proto . click to view it At a high level</strong></summary>

  <div markdown="1">

  ```markdown
  ResourceMetrics
  ‚îî‚îÄ‚îÄ resource (service attributes)
  ‚îî‚îÄ‚îÄ scope_metrics
        ‚îî‚îÄ‚îÄ metrics[]
              ‚îú‚îÄ‚îÄ name
              ‚îú‚îÄ‚îÄ description
              ‚îú‚îÄ‚îÄ unit
              ‚îî‚îÄ‚îÄ data (one of:)
                  ‚Ä¢ Gauge (instant value)
                  ‚Ä¢ Sum (counter / monotonic / non-monotonic)
                  ‚Ä¢ Histogram (bucketed distribution)
                  ‚Ä¢ ExponentialHistogram
                  ‚Ä¢ Summary (quantiles, deprecated)

  ```
  {: file=metrics.proto}

  </div> 
</details>

<details>
  <summary><strong>Here‚Äôs how the above look when serialized as JSON.Metric (Histogram Example)[click to view]</strong></summary>

  <div markdown="1">

  ```json
  {
  "resourceMetrics": [
    {
      "resource": {
        "attributes": [
          { "key": "service.name", "value": { "stringValue": "checkout-service" } }
        ]
      },
      "scopeMetrics": [
        {
          "scope": { "name": "otel.instrumentation.runtime" },
          "metrics": [
            {
              "name": "http.server.duration",
              "description": "Duration of HTTP requests",
              "unit": "ms",
              "histogram": {
                "dataPoints": [
                  {
                    "attributes": [
                      { "key": "http.method", "value": { "stringValue": "GET" } }
                    ],
                    "startTimeUnixNano": "1692288000000000000",
                    "timeUnixNano": "1692288001000000000",
                    "count": "3",
                    "sum": 120.0,
                    "bucketCounts": ["1", "2"],
                    "explicitBounds": [50.0]
                  }
                ]
              }
            }
          ]
        }
      ]
    }
  ]
}
  ```

  </div> 
</details>

>üëâ Metrics are more diverse because telemetry can mean counts, gauges, histograms, etc.
>But again ‚Äî this schema is fixed across OTel.


### Logs üìú ‚Äî *What was said* {#logs-anchor}

Logs are **discrete, timestamped text records** that describe events, messages, errors, or system states.  
They‚Äôre highly detailed, **context-rich**, and excellent for **debugging**, **auditing**, and **forensic analysis**.

- What errors occurred during a specific request?
- What did the service try to do before it failed?
- What was the payload of the incoming API call?
- Discrete, timestamped records of events or errors.
- Can include raw text and/or structured attributes (e.g., key-value pairs).

<span id="log-schema"></span>
**Log Schema `logs.proto`**
  
<details>
  <summary><strong>The canonical structure is defined in file logs.proto. click to view it At a high level:</strong></summary>

  <div markdown="1">

  ```markdown
  ResourceLogs
  ‚îî‚îÄ‚îÄ resource (service attributes)
  ‚îî‚îÄ‚îÄ scope_logs
      ‚îî‚îÄ‚îÄ log_records[]
            ‚îú‚îÄ‚îÄ time_unix_nano
            ‚îú‚îÄ‚îÄ severity_number (enum: TRACE, DEBUG, INFO, WARN, ERROR, FATAL)
            ‚îú‚îÄ‚îÄ severity_text (string version, e.g., "ERROR")
            ‚îú‚îÄ‚îÄ body (actual log message, AnyValue type)
            ‚îú‚îÄ‚îÄ attributes[] (extra metadata)
            ‚îú‚îÄ‚îÄ trace_id (to link log ‚Üí trace)
            ‚îú‚îÄ‚îÄ span_id (to link log ‚Üí span)

  ```
  {: file=logs.proto}

  </div> 
</details>

<details>
  <summary><strong>Here‚Äôs how the above look when serialized as JSON.[click to view]</strong></summary>

  <div markdown="1">

  ```json
  {
  "resourceLogs": [
    {
      "resource": {
        "attributes": [
          { "key": "service.name", "value": { "stringValue": "checkout-service" } }
        ]
      },
      "scopeLogs": [
        {
          "scope": { "name": "otel.instrumentation.logger" },
          "logRecords": [
            {
              "timeUnixNano": "1692288002000000000",
              "severityNumber": "SEVERITY_NUMBER_ERROR",
              "severityText": "ERROR",
              "body": { "stringValue": "User not found: id=42" },
              "attributes": [
                { "key": "exception.type", "value": { "stringValue": "NotFoundError" } }
              ],
              "traceId": "4fd0c9e6e9c6bfb1a5f7d29d6c72b78f",
              "spanId": "6f9c2d7f8c4d1f32"
            }
          ]
        }
      ]
    }
  ]
}
  ```

  </div> 
</details>

> **Notice:**  
> Logs can carry `trace_id` and `span_id`.  
> That‚Äôs how **logs**, **metrics**, and **traces** correlate in OpenTelemetry.

## Conclusion

OpenTelemetry brings **structure**, **consistency**, and **interoperability** to the chaotic world of observability.

It allows developers to:

- **Instrument once, export anywhere** ‚Äî no vendor lock-in
- Use **one schema** across all telemetry types (`traces`, `metrics`, `logs`)
- Automatically benefit from SDKs that abstract away schema complexity
- Connect all signals (e.g., link logs to traces via `trace_id` and `span_id`)
- Standardize telemetry across microservices, languages, and platforms

By adopting OpenTelemetry, you build a **future-proof** and **portable observability stack** ‚Äî decoupled from any specific vendor, but compatible with all.

Whether you're building for **debugging**, **monitoring**, or **business intelligence**, OTel helps you generate **clear, structured telemetry** that every system can understand.